our Bangla Text Dataset is a treasure trove of social media comments, meticulously tagged with sentiments such as 'ironical' and 'sincere'. 
This rich resource allows us to delve deep into the nuances of language and understand the underlying sentiments with unprecedented accuracy.
We complemented this dataset with FactWatch Data, providing invaluable context and insights from fact-checking reports.
ToxLex Data, which helps us refine our understanding of toxicity levels in content, ensuring a more nuanced analysis. the TF-IDF Vectorizer, an essential component from Scikit-learn, enables us to convert text data into a matrix of features, unlocking the power of natural language processing. 
And of course, Pandas stands by our side, facilitating seamless data manipulation and analysis. In our development environment of choice, in Jupyter Notebook, we bring it all together. 
libraries like NumPy for numerical operations, and Matplotlib/Seaborn for visually stunning visualizations of our data and results.
Bidirectional Encoder Representations from Transformers, a state-of-the-art NLP model developed by Google, Known for its deep understanding of language contexts and nuances, BERT takes our analysis to new heights, allowing us to unlock insights that were previously beyond reach.
NLTK/Regex for Python: For text preprocessing tasks such as tokenization and removal of stopwords. Used for coding, visualizing, and documenting the project workflow interactively.
Matplotlib/Seaborn: For generating visualizations of data and results.
We used datasets from Mendeley and github for both training and test the model.
